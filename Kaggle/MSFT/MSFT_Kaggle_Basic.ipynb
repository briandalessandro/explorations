{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pickle\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datadir = '~/data/'\n",
    "\n",
    "train_dev = datadir + 'train_rand_10.csv'\n",
    "train = datadir + 'train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train)\n",
    "train_df.set_index('MachineIdentifier', inplace=True)\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bias_df = pd.read_csv(datadir + 'train_bias_weights.csv')\n",
    "bias_df.set_index('MachineIdentifier', inplace=True)\n",
    "bias_df = bias_df[(bias_df.prob>0.6)]\n",
    "\n",
    "train_df = train_df.merge(bias_df, how='inner', left_index=True, right_index=True)\n",
    "train_df = train_df.drop('prob', axis=1)\n",
    "train_df.shape,bias_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = 'HasDetections'\n",
    "\n",
    "#Organize columns into groups by type\n",
    "x_numeric = list(train_df.describe().columns.values)\n",
    "x_ident = [x for x in x_numeric if 'Identifier' in x] #Some numeric columns actually IDs\n",
    "x_numeric = list(set(x_numeric) - set(x_ident)) #Keep only true numerics\n",
    "x_str = list(set(train_df.columns.values) - set(x_numeric))\n",
    "x_numeric.remove(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(x_numeric), len(x_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to do:\n",
    "- create string and numeric matrices\n",
    "- impute missing values for numeric\n",
    "- create string transformations \n",
    "- feature selection\n",
    "- modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This block will be data transformations only - to be packaged up into a function\n",
    "'''\n",
    "from sklearn.metrics  import mutual_info_score\n",
    "\n",
    "#Helper functions\n",
    "\n",
    "def cond_join(c):\n",
    "    if len(c[1])==0:\n",
    "        return c[0]\n",
    "    else:\n",
    "        return '{}_{}'.format(c[0],c[1])\n",
    "    \n",
    "def entropy(p):\n",
    "    return -1*(p*np.log(p) + (1-p)*np.log((1-p)))\n",
    "\n",
    "base_rate_entropy = entropy(train_df[y].mean())\n",
    "\n",
    "def rel_row_entropy(row):\n",
    "    e1 = entropy(row.HasDetections['mean'])\n",
    "    return (base_rate_entropy - e1) / base_rate_entropy\n",
    "\n",
    "\n",
    "#feature transformation functions\n",
    "\n",
    "def clean_numeric(df, filler=None):\n",
    "    #Clean missing values in numeric data\n",
    "    df_numeric = df[x_numeric]\n",
    "\n",
    "    if filler is None:\n",
    "        medians = df_numeric.median()\n",
    "    else:\n",
    "        medians = filler\n",
    "    \n",
    "    df_numeric.fillna(medians, inplace=True)\n",
    "    return df_numeric, medians\n",
    "\n",
    "def get_booleans(df, y, x, rel_thresh=0.001, n_thresh=2000):\n",
    "    '''\n",
    "    Feature construction/selection:\n",
    "    - compute entropy in each category. \n",
    "    - If relative entropy is above a threshold, convert to binary\n",
    "    - check to make sure no more than K-1 features\n",
    "\n",
    "    '''\n",
    "    df_grp_x = df[[x,y]].groupby(x).agg([len, np.mean]).reset_index()\n",
    "    df_grp_x['relent'] = df_grp_x.apply(rel_row_entropy, axis=1)\n",
    "    df_grp_x.columns = [cond_join(c) for c in df_grp_x.columns.values]\n",
    "    df_grp_x_filt = df_grp_x[(df_grp_x.relent>rel_thresh) & (df_grp_x.HasDetections_len>n_thresh)]\n",
    "    n_cats = df_grp_x.shape[0]\n",
    "    chosen = list(df_grp_x_filt[x].values)\n",
    "    if n_cats == len(chosen):\n",
    "        return chosen[:-1]\n",
    "    else:\n",
    "        return chosen\n",
    "    \n",
    "def get_booleans_all_x(df):\n",
    "    lab = 'MachineIdentifier'\n",
    "    str_trans = {}\n",
    "\n",
    "    for x in x_str:\n",
    "        if x != lab:\n",
    "            bools = get_booleans(df, y, x)\n",
    "            loc_dict = {}\n",
    "            for i, b in enumerate(bools):\n",
    "                loc_dict[i] = b        \n",
    "            str_trans[x] = loc_dict\n",
    "    return str_trans\n",
    "\n",
    "def transform_strings(df, str_trans):\n",
    "    #Now create the string matrix\n",
    "    df_str = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    \n",
    "    for x in str_trans:\n",
    "        if len(x) > 0:\n",
    "            loc_dict = str_trans[x]\n",
    "            for i in range(len(loc_dict)):\n",
    "                df_str[x+'_{}'.format(i)] = 1*(df[x]==loc_dict[i])\n",
    "    return df_str\n",
    "        \n",
    "#Training \n",
    "def build_train_data(train_df):\n",
    "    #train_df = pd.read_csv(train_dev)\n",
    "    #train_df.set_index('MachineIdentifier', inplace=True)\n",
    "    str_trans_train = get_booleans_all_x(train_df)\n",
    "    train_df_str = transform_strings(train_df, str_trans_train)\n",
    "    train_df_num, fillna_train = clean_numeric(train_df)\n",
    "    train_all_x = train_df_num.merge(train_df_str, how='inner', left_index=True, right_index=True)\n",
    "    train_y = train_df[y]\n",
    "    return train_all_x, train_y, str_trans_train, fillna_train\n",
    "    \n",
    "#Test\n",
    "def build_test_data(test_file, fillna_train, str_trans_train):\n",
    "    test_df = pd.read_csv(test_file)\n",
    "    test_df.set_index('MachineIdentifier', inplace=True)\n",
    "    test_df_str = transform_strings(test_df, str_trans_train)\n",
    "    test_df_num, fillna_train = clean_numeric(test_df, fillna_train)\n",
    "    test_all_x = test_df_num.merge(test_df_str, how='inner', left_index=True, right_index=True)\n",
    "    return test_all_x\n",
    "\n",
    "class BestModelPackage(object):\n",
    "    \n",
    "    def __init__(self, model, features, fillna_values, str_transforms):\n",
    "        self.model = model\n",
    "        self.features = features\n",
    "        self.fillna_values = fillna_values\n",
    "        self.str_transforms = str_transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Actual Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_all_x, train_y, str_trans_train, fillna_train = build_train_data(train_df)\n",
    "train_all_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd stage feature selection\n",
    "\n",
    "- partition the data into multiple sets\n",
    "- get marginal info gain (via RF) for each partition\n",
    "- keep features that meet threshold in each partition\n",
    "- take the intersection of all partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_rf_featimp(df_x, df_y):\n",
    "    rf = RandomForestClassifier(n_estimators=100).fit(df_x, df_y)\n",
    "    return rf.feature_importances_\n",
    "\n",
    "def get_robust_featimp(feat_importances, mi_thresh=0.98):\n",
    "    \n",
    "    for i in range(len(feat_importances)):\n",
    "        fi_sort = feat_importances[i][np.argsort(feat_importances[i])][::-1]\n",
    "        keep_n = (np.cumsum(fi_sort)<mi_thresh).sum()\n",
    "        keep_indices = np.argsort(feat_importances[i])[-keep_n:]\n",
    "        best_feats = train_all_x.columns.values[keep_indices]\n",
    "        \n",
    "        if i==0:\n",
    "            best_set = set(best_feats)\n",
    "        else:\n",
    "            best_set = best_set & set(best_feats)\n",
    "            \n",
    "    return list(best_set)\n",
    "        \n",
    "parts = 4\n",
    "def hash_index(row):\n",
    "    return hash(row.name) % parts\n",
    "\n",
    "index_partitions = train_all_x.apply(hash_index, axis=1)\n",
    "\n",
    "feat_imp_dict = {}\n",
    "for i in range(parts):\n",
    "    print(i)\n",
    "    df_x = train_all_x[(index_partitions == i)]\n",
    "    df_y = train_y[(index_partitions == i)]\n",
    "    feat_imp_dict[i] = get_rf_featimp(df_x, df_y)\n",
    "    \n",
    "best_featset = get_robust_featimp(feat_imp_dict, 0.9999)\n",
    "len(best_featset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Pipeline\n",
    "- Run with two validations\n",
    "- Cross validation with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Split data into 80/10/10 for training, validating, test\n",
    "parts = 100\n",
    "def hash_index(row):\n",
    "    return hash(row.name) % parts\n",
    "\n",
    "index_partitions = train_all_x.apply(hash_index, axis=1)\n",
    "\n",
    "holdout_n = min(200000, round(train_all_x.shape[0]*.1))\n",
    "partition_n = int(np.floor(parts * (holdout_n / train_all_x.shape[0])))\n",
    "part_1 = parts - partition_n\n",
    "part_2 = parts - 2*partition_n\n",
    "\n",
    "filt_train = (index_partitions < part_2)\n",
    "filt_val = (index_partitions >= part_2) & (index_partitions < part_1)\n",
    "filt_test = (index_partitions >= part_1)\n",
    "\n",
    "trainsamp_x = train_all_x[filt_train]\n",
    "trainsamp_y = train_y[filt_train]\n",
    "\n",
    "valsamp_x = train_all_x[filt_val]\n",
    "valsamp_y = train_y[filt_val]\n",
    "\n",
    "testsamp_x = train_all_x[filt_test]\n",
    "testsamp_y = train_y[filt_test]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do training here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pg_rf_d = {'n_estimators':[200, 500]}\n",
    "\n",
    "pg_gbdt_d = {'n_estimators':[100, 200],\n",
    "           'learning_rate':[0.05, 0.1],\n",
    "           'max_depth':[7]}\n",
    "\n",
    "pg_rf = ParameterGrid(pg_rf_d)\n",
    "pg_gbdt = ParameterGrid(pg_gbdt_d)\n",
    "\n",
    "aucs = []\n",
    "\n",
    "for g in pg_rf:\n",
    "    print(g)\n",
    "    rf = RandomForestClassifier(**g)\n",
    "    rf.fit(trainsamp_x[best_featset], trainsamp_y)\n",
    "    pred = rf.predict_proba(valsamp_x[best_featset])[:,1]\n",
    "    aucs.append((roc_auc_score(valsamp_y.values,pred), 'rf',g))\n",
    "    rf = None\n",
    "\n",
    "\n",
    "for g in pg_gbdt:\n",
    "    print(g)\n",
    "    gb = GradientBoostingClassifier(**g)\n",
    "    gb.fit(trainsamp_x[best_featset], trainsamp_y)\n",
    "    pred = gb.predict_proba(valsamp_x[best_featset])[:,1]\n",
    "    aucs.append((roc_auc_score(valsamp_y.values,pred), 'gb',g))\n",
    "    gp = None\n",
    "    \n",
    "trainsamp_x = None\n",
    "trainsamp_y = None\n",
    "valsamp_x = None\n",
    "valsamp_y = None\n",
    "testsamp_x = None\n",
    "testsamp_y = None\n",
    "train_df = None\n",
    "\n",
    "print(aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get the best model and save it\n",
    "print('Starting the final model')\n",
    "\n",
    "aucs.sort(reverse=True)\n",
    "best_auc, best_algo, best_params = aucs[0]\n",
    "\n",
    "print(best_algo)\n",
    "print(best_params)\n",
    "\n",
    "bm_pack_pre = BestModelPackage(None, best_featset, fillna_train, str_trans_train)\n",
    "        \n",
    "import pickle\n",
    "\n",
    "modfile = '/Users/briand/data/MSFT_Best_Model_ALL_pre3.pickle'\n",
    "with open(modfile, 'wb') as w:\n",
    "    pickle.dump(bm_pack_pre, w)\n",
    "\n",
    "\n",
    "\n",
    "if best_algo=='rf':\n",
    "    best_model = RandomForestClassifier(**best_params)\n",
    "else:\n",
    "    best_model = GradientBoostingClassifier(**best_params)\n",
    "    \n",
    "best_model.fit(train_all_x[best_featset], train_y)\n",
    "\n",
    "\n",
    "        \n",
    "bm_pack = BestModelPackage(best_model, best_featset, fillna_train, str_trans_train)\n",
    "        \n",
    "import pickle\n",
    "\n",
    "modfile = '/Users/briand/data/MSFT_Best_Model_ALL3.pickle'\n",
    "with open(modfile, 'wb') as w:\n",
    "    pickle.dump(bm_pack, w)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build final model on all training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modfile = '/Users/briand/data/MSFT_Best_Model_ALL3.pickle'\n",
    "\n",
    "with open(modfile, 'rb') as r:\n",
    "    best_mod = pickle.load(r)\n",
    "\n",
    "\n",
    "test_all_x = build_test_data(datadir + 'test.csv', \n",
    "                             best_mod.fillna_values, \n",
    "                             best_mod.str_transforms)\n",
    "\n",
    "test_preds = best_mod.model.predict_proba(test_all_x[best_mod.features])[:,1]\n",
    "\n",
    "test_pred_df = pd.DataFrame(test_preds, index=test_all_x.index.values, \n",
    "                            columns=['HasDetections'])\n",
    "\n",
    "test_pred_df.to_csv(modfile.split('.')[0] + '.csv', sep=',', \n",
    "                    header=True, index=True, index_label='MachineIdentifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is all scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
