{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "DS Caveats: this code works turnkey for the given files at a specific point in time. There are several things that may change in future iterations, such as column names, data schemas and currency exchange rates. Before putting new data into this code, please review it section by section and make sure it conforms to your data's assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir_june = '/Users/briandalessandro/Documents/CrossBoundary/E4I-Datasets/June_2019_DataShare/'\n",
    "survey_data = '1_RF_Baseline_2018-07-24-FINAL.xlsx'\n",
    "payment_data = 'CB_Innovation_Lab_Payment_Data_to_end_of_Jan_2019.csv'\n",
    "consumption_data = 'CB_Innovation_Lab_Consumption_Data_to_end_of_Jan_2019.csv'\n",
    "final_training_data = 'training_all_in.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Parameters\n",
    "\n",
    "These should be reviewed before developing any model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kenya2usd = 0.01 #exchange rate between Kenya and US Dollar\n",
    "tanzania2usd = 0.00044 #exchange rate between tanzanie and US Dollar\n",
    "tariff_start_date = '2016-10-15' #This is a subjective choice, to be made after exploring data\n",
    "target_start_date = '2017-01-01'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The survey data has (2826, 570) (rows,columns)\n"
     ]
    }
   ],
   "source": [
    "survey_df = pd.read_excel(data_dir_june + survey_data)\n",
    "\n",
    "#Clean column names\n",
    "survey_df.columns = [f.replace(' ','_').replace('.','_').replace('__','_').lower() for f in survey_df.columns.values]\n",
    "\n",
    "print('The survey data has {} (rows,columns)'.format(survey_df.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def split_mapper(row_vals, tag_dict):\n",
    "\n",
    "    try:\n",
    "        return ','.join([tag_dict.get(i) for i in row_vals.split(' ')])\n",
    "    except:\n",
    "        return 'none'\n",
    "    \n",
    "\n",
    "def process_cat_question(df, tag):\n",
    "    tag_columns = [f.split('/')[1] for f in df.columns.values if tag in f and f != tag and '/' in f]\n",
    "    tag_columns_drop = [f for f in df.columns.values if tag in f and f != tag]\n",
    "    tag_dict = {str(i+1):str(k) for i,k in enumerate(tag_columns)}\n",
    "    df[tag] = list(map(lambda x:split_mapper(x, tag_dict), df[tag]))\n",
    "    df = df.drop(tag_columns_drop, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def condense_multi_response_columns(df):\n",
    "    '''\n",
    "    Transform the expanded questions into a single field. This makes it easier to do more \n",
    "    general text analysis and feature exploration. This set of steps can be avoided altogether, \n",
    "    but it does clean up the column space nicely. \n",
    "    '''\n",
    "    tags = set([c.split('/')[0] for c in df.columns.values if '/' in c])\n",
    "    non_tags = set(['_47_rank_which_appli_ould_you_like_to_buy','meta','group_xq6xm27',\n",
    "                    'group_hg4pl96','group_ss3zo89','rank_appliances_to_buy'])\n",
    "    tags = list(tags - non_tags)\n",
    "\n",
    "    for tag in tags:\n",
    "        df = process_cat_question(df, tag)\n",
    "\n",
    "    return df, tags\n",
    "\n",
    "\n",
    "\n",
    "def remove_mostly_null_columns(df, missing_threshold=0.8):\n",
    "    '''\n",
    "    Drop any column that has an excess of missing values, determined by miss_thresh\n",
    "    '''\n",
    "    fields = df.columns.values\n",
    "    missing_fields = []\n",
    "    for f in fields:\n",
    "        try:\n",
    "            missing_pct = sum(np.isnan(df[f])) / float(df.shape[0])\n",
    "            if missing_pct > missing_threshold:\n",
    "                missing_fields.append(f)\n",
    "        except:\n",
    "            missing_pct = sum(df[f].isna()) / float(df.shape[0])\n",
    "            if missing_pct > missing_threshold:\n",
    "                missing_fields.append(f)\n",
    "                \n",
    "    df = df.drop(missing_fields, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condense multi-answer columns into a single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results has (2826, 318) (rows,columns)\n"
     ]
    }
   ],
   "source": [
    "survey_df, multi_resp_tags = condense_multi_response_columns(survey_df)\n",
    "\n",
    "print('The results has {} (rows,columns)'.format(survey_df.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove columns with mostly NULL values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results has (2826, 205) (rows,columns)\n"
     ]
    }
   ],
   "source": [
    "survey_df = remove_mostly_null_columns(survey_df, missing_threshold=0.6)\n",
    "print('The results has {} (rows,columns)'.format(survey_df.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change Currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Most of this section is take from Michael's code\n",
    "currency_cols = ['food','water','phone','transportation','health',\n",
    " 'energy','education','debt','savings','rent','hh_expenses',\n",
    " 'hh_income','purchase_of_energy','purchase_of_expensive_items','childrens_education']\n",
    "\n",
    "for col in currency_cols:\n",
    "    country_field = 'country'\n",
    "    survey_df.loc[survey_df[country_field] == 'kenya', col] = survey_df[survey_df[country_field] == 'kenya'][col] * kenya2usd\n",
    "    survey_df.loc[survey_df[country_field] == 'tanzania', col] = survey_df[survey_df[country_field] == 'tanzania'][col] * tanzania2usd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cap Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This is taken from Michael's code, which was copied from intern's code\n",
    "# Age\n",
    "survey_df.loc[survey_df.age < 18,'age']  = 18;\n",
    "\n",
    "# Household_Sizes\n",
    "survey_df.loc[survey_df.hh_size == 0, 'hh_size'] = 1;\n",
    "survey_df.loc[survey_df.hh_size > 15, 'hh_size'] = 15;\n",
    "survey_df.loc[survey_df.hh_size_over_60 > 15,'hh_size_over_60'] = 15;\n",
    "survey_df.loc[survey_df.hh_size_between_5_18 > 15, 'hh_size_between_5_18'] =15;\n",
    "survey_df.loc[survey_df.hh_size_under_5 > 15, 'hh_size_under_5'] = 15;\n",
    "\n",
    "# Rooms\n",
    "survey_df.loc[survey_df.nr_rooms > 10, 'nr_rooms'] = 10\n",
    "survey_df.loc[survey_df.nr_rooms_used_for_sleeping > 10, 'nr_rooms_used_for_sleeping'] = 10\n",
    "survey_df.loc[survey_df.nr_rooms == 0, 'nr_rooms'] = 1\n",
    "survey_df.loc[survey_df.nr_rooms_used_for_sleeping == 0, 'nr_rooms_used_for_sleeping'] = 1\n",
    "\n",
    "#Homework_time\n",
    "# Baseline$Homework_time_hours[Baseline$Homework_time_hours > 16] <- 16\n",
    "survey_df.loc[survey_df.homework_time_hours > 16, 'homework_time_hours'] = 16\n",
    "#school_going_children \n",
    "survey_df.loc[survey_df.schoolgoing_children > 10, 'schoolgoing_children'] = 10\n",
    "# Baseline$Schoolgoing_children[Baseline$Schoolgoing_children > 10] <- 10\n",
    "\n",
    "\n",
    "# fetching times \n",
    "survey_df.loc[survey_df.fetching_time_minutes == 1200, 'fetching_time_minutes'] = 1200/10.\n",
    "survey_df.loc[survey_df.fetching_times_per_day > 10, 'fetching_times_per_day'] = max(survey_df.fetching_times_per_day)/10.;\n",
    "\n",
    "# Baseline$Fetching_times_per_day[Baseline$Fetching_times_per_day > 10] <- max(Baseline$Fetching_times_per_day)/10;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Process Payments Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briandalessandro/anaconda2/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "payments_df = pd.read_csv(data_dir_june + payment_data)\n",
    "consumption_df = pd.read_csv(data_dir_june + consumption_data)\n",
    "\n",
    "payments_df.columns = [f.replace(' ','_').replace('.','_').replace('__','_').lower() for f in payments_df.columns.values]\n",
    "consumption_df.columns = [f.replace(' ','_').replace('.','_').replace('__','_').lower() for f in consumption_df.columns.values]\n",
    "\n",
    "\n",
    "payments_df['month'] = map(lambda s: str(s)[0:7], payments_df.date)\n",
    "consumption_df['month'] = map(lambda s: str(s)[0:7], consumption_df.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get Min/Max Tariffs over each month\n",
    "payments_grp_m = payments_df[(payments_df.date>tariff_start_date)].groupby(['customer_id','month']).sum().reset_index()\n",
    "\n",
    "consumption_grp_m = consumption_df[(consumption_df.date>tariff_start_date)].groupby(['customer_id','month']).sum().reset_index()\n",
    "\n",
    "tariff_df = payments_grp_m[['customer_id','payment_local_curr','month']].merge(consumption_grp_m[['customer_id','consumption_kwh','month']],on=['customer_id','month'])\n",
    "\n",
    "tariff_df['tariff'] = tariff_df['payment_local_curr'] / tariff_df['consumption_kwh'] \n",
    "\n",
    "tariff_df = tariff_df.merge(survey_df[['country','meter_number','developer_code','village_code']],left_on='customer_id',right_on='meter_number')\n",
    "tariff_df.loc[tariff_df[country_field] == 'kenya', 'tariff'] = tariff_df[tariff_df[country_field] == 'kenya']['tariff'] * kenya2usd\n",
    "tariff_df.loc[tariff_df[country_field] == 'tanzania', 'tariff'] = tariff_df[tariff_df[country_field] == 'tanzania']['tariff'] * tanzania2usd\n",
    "\n",
    "clean_filt = (tariff_df['payment_local_curr']>1) & (tariff_df['consumption_kwh']>1)\n",
    "tariff_min_max_df = tariff_df[clean_filt][['customer_id','tariff']].groupby('customer_id').agg([min,max]).reset_index()\n",
    "tariff_min_max_df.columns = ['customer_id','tariff_min','tariff_max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get avg tariff over all months\n",
    "payments_grp = payments_df[(payments_df.date>tariff_start_date)].groupby(['customer_id']).sum().reset_index()\n",
    "consumption_grp = consumption_df[(consumption_df.date>tariff_start_date)].groupby(['customer_id']).sum().reset_index()\n",
    "\n",
    "tariff_df = payments_grp[['customer_id','payment_local_curr']].merge(consumption_grp[['customer_id','consumption_kwh']],on=['customer_id'])\n",
    "\n",
    "tariff_df['tariff'] = tariff_df['payment_local_curr'] / tariff_df['consumption_kwh'] \n",
    "\n",
    "tariff_df = tariff_df.merge(survey_df[['country','meter_number','developer_code','village_code']],left_on='customer_id',right_on='meter_number')\n",
    "\n",
    "tariff_df.loc[tariff_df[country_field] == 'kenya', 'tariff'] = tariff_df[tariff_df[country_field] == 'kenya']['tariff'] * kenya2usd\n",
    "tariff_df.loc[tariff_df[country_field] == 'tanzania', 'tariff'] = tariff_df[tariff_df[country_field] == 'tanzania']['tariff'] * tanzania2usd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get tariffs by village / developer for imputation\n",
    "keys=['developer_code','village_code']\n",
    "\n",
    "payments_grp_d = payments_df[(payments_df.date>tariff_start_date)].groupby(keys).sum().reset_index()\n",
    "consumption_grp_d = consumption_df[(consumption_df.date>tariff_start_date)].groupby(keys).sum().reset_index()\n",
    "\n",
    "tariff_df_d = payments_grp_d[keys+ ['payment_local_curr']].merge(consumption_grp_d[keys+['consumption_kwh']],on=keys)\n",
    "\n",
    "\n",
    "\n",
    "tariff_df_d['tariff_agg'] = tariff_df_d['payment_local_curr'] / tariff_df_d['consumption_kwh'] \n",
    "\n",
    "\n",
    "kenya = 1*(tariff_df_d.developer_code>3)\n",
    "tariff_df_d['tariff_agg'] = tariff_df_d['tariff_agg']*((0.01)*kenya + (0.00044)*(1-kenya))\n",
    "tariff_df_d.columns = [f.lower() for f in tariff_df_d.columns.values]\n",
    "\n",
    "#Not sure about Dev_codes > 5, nonetheless, these aren't in survey data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>payment_local_curr</th>\n",
       "      <th>consumption_kwh</th>\n",
       "      <th>tariff</th>\n",
       "      <th>country</th>\n",
       "      <th>meter_number</th>\n",
       "      <th>developer_code</th>\n",
       "      <th>village_code</th>\n",
       "      <th>tariff_min</th>\n",
       "      <th>tariff_max</th>\n",
       "      <th>tariff_agg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D8-80-39-76-3E-E4</td>\n",
       "      <td>2950.0</td>\n",
       "      <td>86.222</td>\n",
       "      <td>0.342140</td>\n",
       "      <td>kenya</td>\n",
       "      <td>D8-80-39-76-3E-E4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.646959</td>\n",
       "      <td>0.646959</td>\n",
       "      <td>0.646959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D8-80-39-76-3E-F6</td>\n",
       "      <td>3300.0</td>\n",
       "      <td>32.809</td>\n",
       "      <td>1.005822</td>\n",
       "      <td>kenya</td>\n",
       "      <td>D8-80-39-76-3E-F6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.646959</td>\n",
       "      <td>0.646959</td>\n",
       "      <td>0.646959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D8-80-39-76-3E-F8</td>\n",
       "      <td>16800.0</td>\n",
       "      <td>279.576</td>\n",
       "      <td>0.600910</td>\n",
       "      <td>kenya</td>\n",
       "      <td>D8-80-39-76-3E-F8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.646959</td>\n",
       "      <td>0.646959</td>\n",
       "      <td>0.646959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D8-80-39-76-3F-04</td>\n",
       "      <td>17600.0</td>\n",
       "      <td>352.804</td>\n",
       "      <td>0.498861</td>\n",
       "      <td>kenya</td>\n",
       "      <td>D8-80-39-76-3F-04</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.646959</td>\n",
       "      <td>0.646959</td>\n",
       "      <td>0.646959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D8-80-39-76-3F-06</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>74.544</td>\n",
       "      <td>0.603670</td>\n",
       "      <td>kenya</td>\n",
       "      <td>D8-80-39-76-3F-06</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.646959</td>\n",
       "      <td>0.646959</td>\n",
       "      <td>0.646959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         customer_id  payment_local_curr  consumption_kwh    tariff country  \\\n",
       "0  D8-80-39-76-3E-E4              2950.0           86.222  0.342140   kenya   \n",
       "1  D8-80-39-76-3E-F6              3300.0           32.809  1.005822   kenya   \n",
       "2  D8-80-39-76-3E-F8             16800.0          279.576  0.600910   kenya   \n",
       "3  D8-80-39-76-3F-04             17600.0          352.804  0.498861   kenya   \n",
       "4  D8-80-39-76-3F-06              4500.0           74.544  0.603670   kenya   \n",
       "\n",
       "        meter_number  developer_code  village_code  tariff_min  tariff_max  \\\n",
       "0  D8-80-39-76-3E-E4               5             1    0.646959    0.646959   \n",
       "1  D8-80-39-76-3E-F6               5             1    0.646959    0.646959   \n",
       "2  D8-80-39-76-3E-F8               5             1    0.646959    0.646959   \n",
       "3  D8-80-39-76-3F-04               5             1    0.646959    0.646959   \n",
       "4  D8-80-39-76-3F-06               5             1    0.646959    0.646959   \n",
       "\n",
       "   tariff_agg  \n",
       "0    0.646959  \n",
       "1    0.646959  \n",
       "2    0.646959  \n",
       "3    0.646959  \n",
       "4    0.646959  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now put it all together\n",
    "tariff_df = tariff_df.merge(tariff_min_max_df,on='customer_id',how='left').merge(tariff_df_d[['developer_code','village_code','tariff_agg']], on=keys, how='left')\n",
    "\n",
    "\n",
    "#Impute some of the crazy or missing values, using village aggregate\n",
    "tariff_df.loc[tariff_df['consumption_kwh']<1, 'tariff'] = tariff_df[tariff_df['consumption_kwh']<1]['tariff_agg'] \n",
    "tariff_df.loc[tariff_df.tariff==np.Inf, 'tariff'] = tariff_df[tariff_df.tariff==np.Inf]['tariff_agg'] \n",
    "tariff_df.loc[np.isnan(tariff_df.tariff), 'tariff'] = tariff_df[np.isnan(tariff_df.tariff)]['tariff_agg'] \n",
    "tariff_df.loc[np.isnan(tariff_df.tariff_min), 'tariff_min'] = tariff_df[np.isnan(tariff_df.tariff_min)]['tariff_agg']\n",
    "tariff_df.loc[np.isnan(tariff_df.tariff_max), 'tariff_max'] = tariff_df[np.isnan(tariff_df.tariff_max)]['tariff_agg'] \n",
    "tariff_df.loc[np.isnan(tariff_df.tariff), 'tariff']= tariff_df.tariff.median()\n",
    "\n",
    "p99 = np.percentile(tariff_df.tariff,q=99)\n",
    "tariff_df.loc[tariff_df.tariff>p99, 'tariff'] = p99\n",
    "\n",
    "tariff_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Build Target Data\n",
    "\n",
    "Use 3-12 months for each individual, with outliers removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briandalessandro/anaconda2/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cons_df = pd.read_csv(data_dir_june + consumption_data)\n",
    "\n",
    "cons_df.columns = [f.replace(' ','_').replace('.','_').replace('__','_').lower() for f in cons_df.columns.values]\n",
    "\n",
    "\n",
    "keep_fields = ['date','customer_id','consumption_kwh']\n",
    "\n",
    "group1_df = cons_df[keep_fields].groupby(['date','customer_id']).agg([sum]).reset_index()\n",
    "group1_df.columns = ['date','customer_id','consumption_kwh']\n",
    "group1_df['date'] = pd.to_datetime(group1_df['date'])\n",
    "\n",
    "#Get first day for each customer and compute tenure for each entry\n",
    "first_day_df = group1_df[['customer_id','date']].groupby(['customer_id']).min().reset_index()\n",
    "first_day_df.rename({'date':'first_date'}, axis=1, inplace=True)\n",
    "group1_df = group1_df.merge(first_day_df, on='customer_id')\n",
    "group1_df['tenure'] = (group1_df['date'] - group1_df['first_date']).dt.days\n",
    "\n",
    "#We don't trust anything before 2017\n",
    "group1_df = group1_df[(group1_df.date>=target_start_date)]\n",
    "\n",
    "#Get all billings between first 90 days and 1 year\n",
    "group1_df = group1_df[(group1_df.tenure>90) & (group1_df.tenure<=365)]\n",
    "\n",
    "#Get min/max billing dates within this range\n",
    "group1_min_max = group1_df[['customer_id','date']].groupby(['customer_id']).agg([min,max]).reset_index()\n",
    "group1_min_max.columns = ['customer_id', 'start_date','end_date']\n",
    "\n",
    "#Get total usage\n",
    "group1_agg = group1_df[['customer_id','consumption_kwh']].groupby(['customer_id']).sum().reset_index()\n",
    "group1_agg = group1_agg.merge(group1_min_max, on='customer_id')\n",
    "group1_agg['days'] = (group1_agg['end_date'] - group1_agg['start_date']).dt.days + 1\n",
    "group1_agg['avg_consumption'] = group1_agg['consumption_kwh'] / group1_agg['days']\n",
    "\n",
    "#Final Target Data\n",
    "target_df = group1_agg[['customer_id','start_date','end_date','avg_consumption']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Together All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model data has (1859, 212) (rows,columns)\n"
     ]
    }
   ],
   "source": [
    "tariff_df.columns = [f.replace(' ','_').replace('.','_').lower() for f in tariff_df.columns.values]\n",
    "target_df.columns = [f.replace(' ','_').replace('.','_').lower() for f in target_df.columns.values]\n",
    "\n",
    "\n",
    "model_df = survey_df.merge(tariff_df[['customer_id','tariff','tariff_min','tariff_max']], left_on='meter_number',right_on='customer_id')\n",
    "\n",
    "model_df = model_df.merge(target_df, on='customer_id')\n",
    "\n",
    "#Clean features one more time, adding in the '/' as well\n",
    "model_df.columns = [f.replace('/','_').replace(' ','_').replace('.','_').lower() for f in model_df.columns.values]\n",
    "\n",
    "print('The model data has {} (rows,columns)'.format(model_df.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Developer note:</b> I took everything up until this point and fed it into the SparkBeyond platform. The goal of which was to identify if any interesting features could be discovered. In particular, looking for interesting transformations of text or geo-stamped fields. The section below is the set of features that were found to be predictive. But everything above can constitue 'raw' inputs, upon which future feature engineering can be done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Added after Automatic Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def contain_in_list_values(column, value):\n",
    "    return [int(value in f.lower()) for f in column]\n",
    "\n",
    "drop_list = []\n",
    "\n",
    "#Straightforward ones\n",
    "model_df['has_decoder_energy_source'] = 1*(model_df.decoder_energy_source != \"none\") \n",
    "model_df['decoder_energy_source_is_electricity'] = 1*(model_df.decoder_energy_source == \"Electricity\") \n",
    "\n",
    "\n",
    "model_df['energy_source_for_cooking_is_coal'] = 1*(model_df.energy_source_for_cooking == \"coal\") #Note - just coal, worth an investigation\n",
    "model_df['energy_source_for_cooking_not_none'] = 1*(model_df.energy_source_for_cooking != \"none\") #Note - just coal, worth an investigation\n",
    "\n",
    "\n",
    "model_df['hh_source_of_income_is_corn'] = 1*(model_df.hh_source_of_income == \"corn\") \n",
    "model_df['hh_source_of_income_has_commerce'] = contain_in_list_values(model_df.hh_source_of_income,\"commerce\")\n",
    "\n",
    "\n",
    "model_df['energy_source_for_lighting_is_electricity'] = 1*(model_df.energy_source_for_lighting == \"Electricity\") \n",
    "model_df['energy_source_for_lighting_has_solar'] = contain_in_list_values(model_df.energy_source_for_lighting,\"solar\") #this amounts to higher electricity usage interestingly enough\n",
    "model_df['energy_source_for_lighting_is_none'] = 1*(model_df.energy_source_for_lighting == \"none\") \n",
    "\n",
    "\n",
    "model_df['transport_use_or_ownership_has_motorbike'] = contain_in_list_values(model_df.transport_use_or_ownership, \"motorbike\")\n",
    "model_df['transport_use_or_ownership_has_car'] = contain_in_list_values(model_df.transport_use_or_ownership, \"car\")\n",
    "\n",
    "model_df['group_involvement_not_none'] = 1*(model_df.group_involvement != \"none\")\n",
    "\n",
    "model_df['uses_of_non_self_generated_electricity_has_fan'] = contain_in_list_values(model_df.uses_of_non_self_generated_electricity, \"fan\")\n",
    "model_df['uses_of_non_self_generated_electricity_has_phone_charging'] = contain_in_list_values(model_df.uses_of_non_self_generated_electricity, \"phone_charging\")\n",
    "model_df['uses_of_non_self_generated_electricity_count'] = [len(f.split(','))*(f!='none') for f in model_df.uses_of_non_self_generated_electricity]\n",
    "model_df['uses_of_non_self_generated_electricity_has_radio'] = contain_in_list_values(model_df.uses_of_non_self_generated_electricity, \"radio\")\n",
    "\n",
    "\n",
    "model_df['occupation_not_none'] = 1*(model_df.occupation != \"none\")\n",
    "\n",
    "model_df['transport_use_or_ownership_has_car'] = contain_in_list_values(model_df.transport_use_or_ownership, \"car\")\n",
    "\n",
    "model_df['income_generating_activity_has_salary_work'] = contain_in_list_values(model_df.income_generating_activity, \"salary_work\")\n",
    "\n",
    "\n",
    "model_df['how_often_in_the_e_following_feelings_is_ok'] = 1*(model_df.how_often_in_the_e_following_feelings == 'OK')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model data has (1859, 192) (rows,columns)\n"
     ]
    }
   ],
   "source": [
    "#These vars when present have the value OK - convert these to binary\n",
    "cat_vars = ['how_often_in_the_e_following_feelings',\n",
    " 'group_hg4pl96_group_xl3ae56__20_how_many_of_each_s_your_household_own',\n",
    " 'group_hg4pl96_group_ti5qr72__25_who_makes_the_fo_ion_in_the_household',\n",
    " 'agree_or_disagree_wi_following_statements',\n",
    " 'group_xq6xm27_group_cz2uj23_group_wh9vo30__35_how_many_of_each_light_your_household',\n",
    " 'group_hg4pl96_group_cr5xn88__24_how_much_does_yo_ing_local_currency',\n",
    " 'group_hg4pl96_group_ob7yj27_the_households_total_per_month_on_average']\n",
    "\n",
    "\n",
    "for v in cat_vars:\n",
    "    model_df[v] = 1*(model_df[v]=='OK')\n",
    "\n",
    "    \n",
    "#Simple transformations\n",
    "def f(x, offset=0): \n",
    "    try:\n",
    "        slp = int(x[0:2]) + int(x[3:5]) / 60.0\n",
    "        if slp < 8:\n",
    "            slp += offset\n",
    "        return slp   \n",
    "    except:\n",
    "        return np.NaN\n",
    "\n",
    "\n",
    "model_df['sleep_time'] = list(map(lambda x: f(x,24), model_df['sleep_time'].values))\n",
    "model_df['wake_up_time'] = list(map(lambda x: f(x), model_df['wake_up_time'].values))\n",
    "\n",
    "\n",
    "model_df = model_df.drop(multi_resp_tags, axis=1)\n",
    "\n",
    "print('The model data has {} (rows,columns)'.format(model_df.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Value Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#These we just fill missing values with 0\n",
    "fill_zero_list = ['good','bad','afraid','angry']\n",
    "\n",
    "for v in fill_zero_list:\n",
    "    model_df[v] = model_df[v].fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briandalessandro/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:27: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n"
     ]
    }
   ],
   "source": [
    "#These are features that won't go into a model, so don't need imputation\n",
    "non_model_features = ['start_date','submission_time','end_date','device_id','country','developer_code', \n",
    "                      'village_code', 'joint_code', 'gps_coordinates',\n",
    "                       'gps_coordinates_latitude', 'gps_coordinates_longitude',\n",
    "                       'gps_coordinates_altitude', 'gps_coordinates_precision',\n",
    "                       'meter_number','survey_version','meta_instanceid', 'id', 'uuid', 'index', \n",
    "                       'parent_index']\n",
    "\n",
    "fill_columns = list(set(model_df.columns.values) - set(non_model_features))\n",
    "\n",
    "\n",
    "def get_missval_signal(df, var):\n",
    "    '''\n",
    "    get a t-stat on the difference in the target variable for missing vs non missing\n",
    "    '''\n",
    "    df2 = pd.DataFrame({'x':df[var].isna(), 'y':df.avg_consumption})\n",
    "    dfg = df2.groupby('x').agg([len,np.mean,np.std]).reset_index()\n",
    "    dfg.columns = ['x', 'n','mu','sig']\n",
    "    dfg_f = dfg[(dfg.x==False)]\n",
    "    dfg_t = dfg[(dfg.x==True)]\n",
    "    return (dfg_t.mu.values - dfg_f.mu.values) / np.sqrt(dfg_t.sig.values**2 / dfg_t.n.values + dfg_f.sig.values**2 / dfg_f.n.values)\n",
    "\n",
    "\n",
    "mv_has_signal = []\n",
    "mv_str_type = []\n",
    "for var in fill_columns:\n",
    "    if np.abs(get_missval_signal(model_df, var)) > 2.2: #roughly 97.5% two tailed statistical significance\n",
    "        mv_has_signal.append(var)\n",
    "        model_df[var + '_is_missing'] = 1*model_df[var].isna()\n",
    "        \n",
    "    is_str = type(model_df[var][1-(model_df[var].isna())].values[0]) in [str]\n",
    "        \n",
    "    #Convert numerics to median value\n",
    "    if not is_str:\n",
    "        model_df[var] =  model_df[var].fillna(model_df[var].median(skipna=True))\n",
    "        \n",
    "    else:\n",
    "        mv_str_type.append(var)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_df.to_csv(data_dir_june + final_training_data, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
